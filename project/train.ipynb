{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n",
      "Epoch 10/1000: Loss = 302.110291\n",
      "Epoch 20/1000: Loss = 147.956573\n",
      "Epoch 30/1000: Loss = 101.997650\n",
      "Epoch 40/1000: Loss = 81.422707\n",
      "Epoch 50/1000: Loss = 70.404388\n",
      "Epoch 60/1000: Loss = 60.871868\n",
      "Epoch 70/1000: Loss = 56.565331\n",
      "Epoch 80/1000: Loss = 54.044228\n",
      "Epoch 90/1000: Loss = 51.575279\n",
      "Epoch 100/1000: Loss = 48.887615\n",
      "Epoch 110/1000: Loss = 46.421116\n",
      "Epoch 120/1000: Loss = 43.992611\n",
      "Epoch 130/1000: Loss = 41.788727\n",
      "Epoch 140/1000: Loss = 39.550995\n",
      "Epoch 150/1000: Loss = 37.632568\n",
      "Epoch 160/1000: Loss = 35.729435\n",
      "Epoch 170/1000: Loss = 34.152168\n",
      "Epoch 180/1000: Loss = 32.572163\n",
      "Epoch 190/1000: Loss = 31.232347\n",
      "Epoch 200/1000: Loss = 30.100338\n",
      "Epoch 210/1000: Loss = 28.934067\n",
      "Epoch 220/1000: Loss = 27.976254\n",
      "Epoch 230/1000: Loss = 27.201008\n",
      "Epoch 240/1000: Loss = 26.356358\n",
      "Epoch 250/1000: Loss = 25.568775\n",
      "Epoch 260/1000: Loss = 24.904524\n",
      "Epoch 270/1000: Loss = 24.343601\n",
      "Epoch 280/1000: Loss = 23.825855\n",
      "Epoch 290/1000: Loss = 23.169077\n",
      "Epoch 300/1000: Loss = 22.590292\n",
      "Epoch 310/1000: Loss = 22.072969\n",
      "Epoch 320/1000: Loss = 21.623667\n",
      "Epoch 330/1000: Loss = 21.227341\n",
      "Epoch 340/1000: Loss = 20.876480\n",
      "Epoch 350/1000: Loss = 20.406250\n",
      "Epoch 360/1000: Loss = 19.933958\n",
      "Epoch 370/1000: Loss = 19.498579\n",
      "Epoch 380/1000: Loss = 19.112808\n",
      "Epoch 390/1000: Loss = 18.769651\n",
      "Epoch 400/1000: Loss = 18.464054\n",
      "Epoch 410/1000: Loss = 18.190462\n",
      "Epoch 420/1000: Loss = 17.944683\n",
      "Epoch 430/1000: Loss = 17.723049\n",
      "Epoch 440/1000: Loss = 17.409298\n",
      "Epoch 450/1000: Loss = 17.066359\n",
      "Epoch 460/1000: Loss = 16.742907\n",
      "Epoch 470/1000: Loss = 16.454901\n",
      "Epoch 480/1000: Loss = 16.195190\n",
      "Epoch 490/1000: Loss = 15.961058\n",
      "Epoch 500/1000: Loss = 15.748615\n",
      "Epoch 510/1000: Loss = 15.555202\n",
      "Epoch 520/1000: Loss = 15.378294\n",
      "Epoch 530/1000: Loss = 15.215740\n",
      "Epoch 540/1000: Loss = 15.065683\n",
      "Epoch 550/1000: Loss = 14.926547\n",
      "Epoch 560/1000: Loss = 14.796985\n",
      "Epoch 570/1000: Loss = 14.675801\n",
      "Epoch 580/1000: Loss = 14.439243\n",
      "Epoch 590/1000: Loss = 14.205635\n",
      "Epoch 600/1000: Loss = 13.986450\n",
      "Epoch 610/1000: Loss = 13.785728\n",
      "Epoch 620/1000: Loss = 13.599491\n",
      "Epoch 630/1000: Loss = 13.426069\n",
      "Epoch 640/1000: Loss = 13.264552\n",
      "Epoch 650/1000: Loss = 13.113492\n",
      "Epoch 660/1000: Loss = 12.971694\n",
      "Epoch 670/1000: Loss = 12.838132\n",
      "Epoch 680/1000: Loss = 12.711941\n",
      "Epoch 690/1000: Loss = 12.592403\n",
      "Epoch 700/1000: Loss = 12.478899\n",
      "Epoch 710/1000: Loss = 12.370902\n",
      "Epoch 720/1000: Loss = 12.267957\n",
      "Epoch 730/1000: Loss = 12.169662\n",
      "Epoch 740/1000: Loss = 12.075675\n",
      "Epoch 750/1000: Loss = 11.985697\n",
      "Epoch 760/1000: Loss = 11.899454\n",
      "Epoch 770/1000: Loss = 11.816707\n",
      "Epoch 780/1000: Loss = 11.737242\n",
      "Epoch 790/1000: Loss = 11.660864\n",
      "Epoch 800/1000: Loss = 11.587399\n",
      "Epoch 810/1000: Loss = 11.457816\n",
      "Epoch 820/1000: Loss = 11.289957\n",
      "Epoch 830/1000: Loss = 11.130791\n",
      "Epoch 840/1000: Loss = 10.984045\n",
      "Epoch 850/1000: Loss = 10.844023\n",
      "Epoch 860/1000: Loss = 10.711269\n",
      "Epoch 870/1000: Loss = 10.585978\n",
      "Epoch 880/1000: Loss = 10.467275\n",
      "Epoch 890/1000: Loss = 10.354517\n",
      "Epoch 900/1000: Loss = 10.247240\n",
      "Epoch 910/1000: Loss = 10.145024\n",
      "Epoch 920/1000: Loss = 10.047489\n",
      "Epoch 930/1000: Loss = 9.954305\n",
      "Epoch 940/1000: Loss = 9.865177\n",
      "Epoch 950/1000: Loss = 9.779846\n",
      "Epoch 960/1000: Loss = 9.698071\n",
      "Epoch 970/1000: Loss = 9.619638\n",
      "Epoch 980/1000: Loss = 9.544350\n",
      "Epoch 990/1000: Loss = 9.472024\n",
      "Epoch 1000/1000: Loss = 9.402493\n",
      "Training complete.\n",
      "\n",
      "+--------+----------------------------------------------------------+\n",
      "| t      | yÌ‚(t)                                                     |\n",
      "+--------+----------------------------------------------------------+\n",
      "|   0.00 | [0. 0. 0. 0. 0.]                                         |\n",
      "+--------+----------------------------------------------------------+\n",
      "|   2.50 | [0.55843323 0.37774917 1.3395762  0.95628244 2.3415926 ] |\n",
      "+--------+----------------------------------------------------------+\n",
      "|   5.00 | [0.53832877 0.2407866  1.3365941  0.95648557 2.954164  ] |\n",
      "+--------+----------------------------------------------------------+\n",
      "|   7.50 | [0.51960766 0.1977482  1.2940776  0.9593829  3.1136396 ] |\n",
      "+--------+----------------------------------------------------------+\n",
      "|  10.00 | [0.5069148  0.19424045 1.2779634  0.9730445  3.1519907 ] |\n",
      "+--------+----------------------------------------------------------+\n",
      "Model saved to example_1_pinn_1000_2025_02_20_out_vars_5.pt\n",
      "Starting training...\n",
      "Epoch 10/1000: Loss = 19.540941\n",
      "Epoch 20/1000: Loss = 11.554751\n",
      "Epoch 30/1000: Loss = 7.728041\n",
      "Epoch 40/1000: Loss = 6.727319\n",
      "Epoch 50/1000: Loss = 5.929556\n",
      "Epoch 60/1000: Loss = 5.336572\n",
      "Epoch 70/1000: Loss = 4.924288\n",
      "Epoch 80/1000: Loss = 4.660483\n",
      "Epoch 90/1000: Loss = 4.488482\n",
      "Epoch 100/1000: Loss = 4.354662\n",
      "Epoch 110/1000: Loss = 4.228198\n",
      "Epoch 120/1000: Loss = 4.105515\n",
      "Epoch 130/1000: Loss = 3.985790\n",
      "Epoch 140/1000: Loss = 3.865964\n",
      "Epoch 150/1000: Loss = 3.754128\n",
      "Epoch 160/1000: Loss = 3.643757\n",
      "Epoch 170/1000: Loss = 3.524664\n",
      "Epoch 180/1000: Loss = 3.338255\n",
      "Epoch 190/1000: Loss = 3.165720\n",
      "Epoch 200/1000: Loss = 3.022316\n",
      "Epoch 210/1000: Loss = 2.911108\n",
      "Epoch 220/1000: Loss = 2.819907\n",
      "Epoch 230/1000: Loss = 2.733289\n",
      "Epoch 240/1000: Loss = 2.658096\n",
      "Epoch 250/1000: Loss = 2.591600\n",
      "Epoch 260/1000: Loss = 1.626385\n",
      "Epoch 270/1000: Loss = 1.539075\n",
      "Epoch 280/1000: Loss = 1.304220\n",
      "Epoch 290/1000: Loss = 1.212631\n",
      "Epoch 300/1000: Loss = 1.172219\n",
      "Epoch 310/1000: Loss = 1.128893\n",
      "Epoch 320/1000: Loss = 1.086687\n",
      "Epoch 330/1000: Loss = 1.048740\n",
      "Epoch 340/1000: Loss = 1.013032\n",
      "Epoch 350/1000: Loss = 0.979934\n",
      "Epoch 360/1000: Loss = 0.949342\n",
      "Epoch 370/1000: Loss = 0.920892\n",
      "Epoch 380/1000: Loss = 0.893665\n",
      "Epoch 390/1000: Loss = 0.868229\n",
      "Epoch 400/1000: Loss = 0.841183\n",
      "Epoch 410/1000: Loss = 0.813666\n",
      "Epoch 420/1000: Loss = 0.787419\n",
      "Epoch 430/1000: Loss = 0.762919\n",
      "Epoch 440/1000: Loss = 0.740051\n",
      "Epoch 450/1000: Loss = 0.718666\n",
      "Epoch 460/1000: Loss = 0.697928\n",
      "Epoch 470/1000: Loss = 0.678367\n",
      "Epoch 480/1000: Loss = 0.659984\n",
      "Epoch 490/1000: Loss = 0.642662\n",
      "Epoch 500/1000: Loss = 0.626283\n",
      "Epoch 510/1000: Loss = 0.610149\n",
      "Epoch 520/1000: Loss = 0.594725\n",
      "Epoch 530/1000: Loss = 0.580082\n",
      "Epoch 540/1000: Loss = 0.565181\n",
      "Epoch 550/1000: Loss = 0.548776\n",
      "Epoch 560/1000: Loss = 0.532822\n",
      "Epoch 570/1000: Loss = 0.517323\n",
      "Epoch 580/1000: Loss = 0.502382\n",
      "Epoch 590/1000: Loss = 0.488158\n",
      "Epoch 600/1000: Loss = 0.474635\n",
      "Epoch 610/1000: Loss = 0.461770\n",
      "Epoch 620/1000: Loss = 0.449510\n",
      "Epoch 630/1000: Loss = 0.437805\n",
      "Epoch 640/1000: Loss = 0.426614\n",
      "Epoch 650/1000: Loss = 0.415638\n",
      "Epoch 660/1000: Loss = 0.404905\n",
      "Epoch 670/1000: Loss = 0.394645\n",
      "Epoch 680/1000: Loss = 0.384832\n",
      "Epoch 690/1000: Loss = 0.375445\n",
      "Epoch 700/1000: Loss = 0.366459\n",
      "Epoch 710/1000: Loss = 0.357846\n",
      "Epoch 720/1000: Loss = 0.349588\n",
      "Epoch 730/1000: Loss = 0.341663\n",
      "Epoch 740/1000: Loss = 0.334057\n",
      "Epoch 750/1000: Loss = 0.326735\n",
      "Epoch 760/1000: Loss = 0.390020\n",
      "Epoch 770/1000: Loss = 0.346233\n",
      "Epoch 780/1000: Loss = 0.319395\n",
      "Epoch 790/1000: Loss = 0.300667\n",
      "Epoch 800/1000: Loss = 0.294465\n",
      "Epoch 810/1000: Loss = 0.288826\n",
      "Epoch 820/1000: Loss = 0.282732\n",
      "Epoch 830/1000: Loss = 0.276675\n",
      "Epoch 840/1000: Loss = 0.269918\n",
      "Epoch 850/1000: Loss = 0.263343\n",
      "Epoch 860/1000: Loss = 0.257048\n",
      "Epoch 870/1000: Loss = 0.250994\n",
      "Epoch 880/1000: Loss = 0.245225\n",
      "Epoch 890/1000: Loss = 0.248370\n",
      "Epoch 900/1000: Loss = 0.269434\n",
      "Epoch 910/1000: Loss = 0.231273\n",
      "Epoch 920/1000: Loss = 0.231415\n",
      "Epoch 930/1000: Loss = 0.219142\n",
      "Epoch 940/1000: Loss = 0.215322\n",
      "Epoch 950/1000: Loss = 0.210827\n",
      "Epoch 960/1000: Loss = 0.206448\n",
      "Epoch 970/1000: Loss = 0.202344\n",
      "Epoch 980/1000: Loss = 0.198475\n",
      "Epoch 990/1000: Loss = 0.194795\n",
      "Epoch 1000/1000: Loss = 0.191394\n",
      "Training complete.\n",
      "\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "| t      | yÌ‚(t)                                                                     |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "|   0.00 | [ 0.  0.  0.  0. -0.  0. -0. -0.  0.]                                    |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "|   2.50 | [ 3.1237304e-01 -3.9636666e-01  1.8842037e+00  1.2004813e+00             |\n",
      "|        |   3.1324896e-01  2.4403534e-03 -1.2783166e-02  8.3727837e-01             |\n",
      "|        |  -1.6393493e-03]                                                         |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "|   5.00 | [ 1.7652248e-01 -2.2535606e-01  1.7903175e+00  1.3024046e+00             |\n",
      "|        |   5.2998316e-01  3.4169771e-04  1.4324912e-02  1.3073922e+00             |\n",
      "|        |  -2.0116747e-03]                                                         |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "|   7.50 | [ 0.18676957 -0.05017485  1.641822    1.2348204   0.6043127  -0.00352385 |\n",
      "|        |   0.00611254  1.4784439   0.00289358]                                    |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "|  10.00 | [ 1.7572083e-01  6.8586990e-02  1.5632167e+00  1.1765540e+00             |\n",
      "|        |   6.4565432e-01  2.8174208e-03 -1.4368822e-02  1.5496203e+00             |\n",
      "|        |  -8.1195717e-04]                                                         |\n",
      "+--------+--------------------------------------------------------------------------+\n",
      "Model saved to example_2_pinn_1000_2025_02_20_out_vars_9.pt\n",
      "Starting training...\n",
      "Epoch 10/1000: Loss = 16.994917\n",
      "Epoch 20/1000: Loss = 13.845007\n",
      "Epoch 30/1000: Loss = 12.767288\n",
      "Epoch 40/1000: Loss = 11.129959\n",
      "Epoch 50/1000: Loss = 10.142839\n",
      "Epoch 60/1000: Loss = 9.555564\n",
      "Epoch 70/1000: Loss = 9.238822\n",
      "Epoch 80/1000: Loss = 9.046948\n",
      "Epoch 90/1000: Loss = 8.875042\n",
      "Epoch 100/1000: Loss = 8.700228\n",
      "Epoch 110/1000: Loss = 8.530714\n",
      "Epoch 120/1000: Loss = 8.361526\n",
      "Epoch 130/1000: Loss = 5.205409\n",
      "Epoch 140/1000: Loss = 3.237629\n",
      "Epoch 150/1000: Loss = 2.881230\n",
      "Epoch 160/1000: Loss = 2.639931\n",
      "Epoch 170/1000: Loss = 2.472640\n",
      "Epoch 180/1000: Loss = 2.305827\n",
      "Epoch 190/1000: Loss = 2.191490\n",
      "Epoch 200/1000: Loss = 2.103166\n",
      "Epoch 210/1000: Loss = 2.013317\n",
      "Epoch 220/1000: Loss = 1.930976\n",
      "Epoch 230/1000: Loss = 1.855534\n",
      "Epoch 240/1000: Loss = 1.788894\n",
      "Epoch 250/1000: Loss = 1.721775\n",
      "Epoch 260/1000: Loss = 1.653364\n",
      "Epoch 270/1000: Loss = 1.590119\n",
      "Epoch 280/1000: Loss = 1.532086\n",
      "Epoch 290/1000: Loss = 1.480711\n",
      "Epoch 300/1000: Loss = 1.432905\n",
      "Epoch 310/1000: Loss = 1.380564\n",
      "Epoch 320/1000: Loss = 1.331067\n",
      "Epoch 330/1000: Loss = 1.284637\n",
      "Epoch 340/1000: Loss = 1.242507\n",
      "Epoch 350/1000: Loss = 1.204999\n",
      "Epoch 360/1000: Loss = 1.170588\n",
      "Epoch 370/1000: Loss = 1.137268\n",
      "Epoch 380/1000: Loss = 1.107058\n",
      "Epoch 390/1000: Loss = 1.075637\n",
      "Epoch 400/1000: Loss = 1.041947\n",
      "Epoch 410/1000: Loss = 1.009261\n",
      "Epoch 420/1000: Loss = 0.979303\n",
      "Epoch 430/1000: Loss = 0.951850\n",
      "Epoch 440/1000: Loss = 0.926664\n",
      "Epoch 450/1000: Loss = 0.903358\n",
      "Epoch 460/1000: Loss = 0.879363\n",
      "Epoch 470/1000: Loss = 0.856430\n",
      "Epoch 480/1000: Loss = 0.835133\n",
      "Epoch 490/1000: Loss = 0.815281\n",
      "Epoch 500/1000: Loss = 0.796723\n",
      "Epoch 510/1000: Loss = 0.779308\n",
      "Epoch 520/1000: Loss = 0.762885\n",
      "Epoch 530/1000: Loss = 0.742032\n",
      "Epoch 540/1000: Loss = 0.719951\n",
      "Epoch 550/1000: Loss = 0.699286\n",
      "Epoch 560/1000: Loss = 0.679884\n",
      "Epoch 570/1000: Loss = 0.661966\n",
      "Epoch 580/1000: Loss = 0.645285\n",
      "Epoch 590/1000: Loss = 0.629720\n",
      "Epoch 600/1000: Loss = 0.615158\n",
      "Epoch 610/1000: Loss = 0.601251\n",
      "Epoch 620/1000: Loss = 0.586375\n",
      "Epoch 630/1000: Loss = 0.645417\n",
      "Epoch 640/1000: Loss = 0.560962\n",
      "Epoch 650/1000: Loss = 0.557191\n",
      "Epoch 660/1000: Loss = 0.542959\n",
      "Epoch 670/1000: Loss = 0.528580\n",
      "Epoch 680/1000: Loss = 0.518209\n",
      "Epoch 690/1000: Loss = 0.509180\n",
      "Epoch 700/1000: Loss = 0.500718\n",
      "Epoch 710/1000: Loss = 0.492700\n",
      "Epoch 720/1000: Loss = 0.485069\n",
      "Epoch 730/1000: Loss = 0.477822\n",
      "Epoch 740/1000: Loss = 0.469388\n",
      "Epoch 750/1000: Loss = 0.461062\n",
      "Epoch 760/1000: Loss = 0.450445\n",
      "Epoch 770/1000: Loss = 0.439670\n",
      "Epoch 780/1000: Loss = 0.429443\n",
      "Epoch 790/1000: Loss = 0.419922\n",
      "Epoch 800/1000: Loss = 0.411046\n",
      "Epoch 810/1000: Loss = 0.403227\n",
      "Epoch 820/1000: Loss = 0.463190\n",
      "Epoch 830/1000: Loss = 0.424987\n",
      "Epoch 840/1000: Loss = 0.396196\n",
      "Epoch 850/1000: Loss = 0.377782\n",
      "Epoch 860/1000: Loss = 0.368966\n",
      "Epoch 870/1000: Loss = 0.363831\n",
      "Epoch 880/1000: Loss = 0.358064\n",
      "Epoch 890/1000: Loss = 0.352811\n",
      "Epoch 900/1000: Loss = 0.347811\n",
      "Epoch 910/1000: Loss = 0.343076\n",
      "Epoch 920/1000: Loss = 0.337365\n",
      "Epoch 930/1000: Loss = 0.331829\n",
      "Epoch 940/1000: Loss = 0.326579\n",
      "Epoch 950/1000: Loss = 0.321617\n",
      "Epoch 960/1000: Loss = 0.316935\n",
      "Epoch 970/1000: Loss = 0.312692\n",
      "Epoch 980/1000: Loss = 0.341683\n",
      "Epoch 990/1000: Loss = 0.355441\n",
      "Epoch 1000/1000: Loss = 0.305332\n",
      "Training complete.\n",
      "\n",
      "+--------+---------------------------------------------------------------+\n",
      "| t      | yÌ‚(t)                                                          |\n",
      "+--------+---------------------------------------------------------------+\n",
      "|   0.00 | [ 0.  0.  0.  0. -0. -0.  0.  0.]                             |\n",
      "+--------+---------------------------------------------------------------+\n",
      "|   2.50 | [-6.5788162e-01  1.7665488e+00  1.7480249e+00  8.6793739e-01  |\n",
      "|        |   7.1199089e-01  1.1830715e+00 -2.3457641e-03 -8.6725355e-05] |\n",
      "+--------+---------------------------------------------------------------+\n",
      "|   5.00 | [-2.9374585e-01  1.4023951e+00  1.7548327e+00  9.9215561e-01  |\n",
      "|        |   1.0053761e+00  1.8762258e+00  1.2493385e-03 -4.8389230e-03] |\n",
      "+--------+---------------------------------------------------------------+\n",
      "|   7.50 | [-9.3972310e-02  1.2693754e+00  1.7214427e+00  1.0363854e+00  |\n",
      "|        |   9.9015766e-01  2.1839030e+00 -1.2188747e-03  1.6137521e-03] |\n",
      "+--------+---------------------------------------------------------------+\n",
      "|  10.00 | [-1.0127606e-02  1.2309563e+00  1.6960728e+00  1.0720834e+00  |\n",
      "|        |   9.7050899e-01  2.3325622e+00  8.5657579e-04  3.3521843e-03] |\n",
      "+--------+---------------------------------------------------------------+\n",
      "Model saved to example_3_pinn_1000_2025_02_20_out_vars_8.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "from colorama import just_fix_windows_console, init, Fore, Style\n",
    "from typing import List, Callable\n",
    "from prettytable import PrettyTable, ALL\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DEVICE (from device.py)\n",
    "# ------------------------------\n",
    "def get_device():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# MODELS (from models.py)\n",
    "# ------------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, out_vars=5):\n",
    "        super(PINN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 100)\n",
    "        self.fc2 = nn.Linear(100, out_vars)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.out_vars = out_vars\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Forward pass for a scalar (or batch) time input.\n",
    "        We assume t is a tensor of shape (N, 1) or a scalar tensor.\n",
    "        The network output is modulated as:\n",
    "            yÌ‚(t) = (1 - exp(-t)) * NN(t)\n",
    "        to enforce yÌ‚(0) = 0.\n",
    "        \"\"\"\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        x = self.activation(self.fc1(t))\n",
    "        out = self.fc2(x)\n",
    "        return (1 - torch.exp(-t)) * out\n",
    "\n",
    "\n",
    "def create_loss(\n",
    "    model: PINN, ts: torch.Tensor, phi: Callable[[torch.Tensor], torch.Tensor]\n",
    "):\n",
    "    def compute_loss_vectorized():\n",
    "        # ts: shape (N,) ; we keep ts as 1D tensor and unsqueeze when needed.\n",
    "        ts_var = ts.clone().detach().requires_grad_(True)\n",
    "        y_hat = model(ts_var.unsqueeze(1))\n",
    "\n",
    "        def model_single(t):\n",
    "            return model(t.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        dy_dt = torch.vmap(torch.func.jacrev(model_single))(\n",
    "            ts_var\n",
    "        )  # shape: (N, out_vars)\n",
    "        phi_y = torch.vmap(phi)(y_hat)\n",
    "        residuals = dy_dt - phi_y\n",
    "        loss = torch.mean(torch.sum(residuals**2, dim=1))\n",
    "        return loss\n",
    "\n",
    "    return compute_loss_vectorized\n",
    "\n",
    "\n",
    "def save_model(model: PINN, name: str = \"pinn_model\"):\n",
    "    current_date = datetime.date.today().strftime(\"%Y_%m_%d\")\n",
    "    filename = f\"{name}_{current_date}_out_vars_{model.out_vars}.pt\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "\n",
    "def train_model(phi, ts, device, lr=0.001, epochs=1000, out_vars=5):\n",
    "    just_fix_windows_console()\n",
    "    init()\n",
    "    model = PINN(out_vars=out_vars).to(device)\n",
    "    compute_loss_vectorized = create_loss(model, ts, phi)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(Fore.RED + \"Starting training...\" + Style.RESET_ALL)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        loss_val = compute_loss_vectorized()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                Fore.MAGENTA\n",
    "                + f\"Epoch {epoch}/{epochs}: Loss = {loss_val.item():.6f}\"\n",
    "                + Style.RESET_ALL\n",
    "            )\n",
    "        if loss_val.item() < 1e-4:\n",
    "            print(\n",
    "                Fore.GREEN\n",
    "                + f\"Stopping training at epoch {epoch} with loss = {loss_val.item():.6f}\"\n",
    "                + Style.RESET_ALL\n",
    "            )\n",
    "            break\n",
    "    print(Fore.BLUE + \"Training complete.\\n\" + Style.RESET_ALL)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(model, device, filename):\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model: PINN, test_times: List[int], device=torch.device(\"cpu\")):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"t\", \"yÌ‚(t)\"]\n",
    "    table.hrules = ALL\n",
    "    table.align[\"t\"] = \"l\"\n",
    "    table.align[\"yÌ‚(t)\"] = \"l\"\n",
    "    for t in test_times:\n",
    "        t_tensor = torch.tensor(\n",
    "            t, dtype=torch.float32, device=device, requires_grad=True\n",
    "        )\n",
    "        y_pred = model(t_tensor)\n",
    "        y_pred_str = str(y_pred.detach().cpu().numpy().flatten())\n",
    "        table.add_row([f\"{t:6.2f}\", y_pred_str])\n",
    "    print(Fore.MAGENTA + table.get_string() + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ODE (from ode.py)\n",
    "# ------------------------------\n",
    "def createPhi(D, A, b):\n",
    "    r, n = A.shape\n",
    "\n",
    "    def phi1(y):\n",
    "        x = y[:n]\n",
    "        u = y[n:]\n",
    "        m = torch.clamp(u + A @ x - b, min=0.0)\n",
    "        top = -(D.unsqueeze(1) + A.t() @ m)\n",
    "        bottom = m - u\n",
    "        return torch.cat((top, bottom), 0).squeeze(1)\n",
    "\n",
    "    def phi2(y):\n",
    "        x = y[:n]\n",
    "        u = y[n:]\n",
    "        m = torch.clamp(u + A @ x - b, min=0.0)\n",
    "        top = -(D + A.t() @ m)\n",
    "        bottom = m - u\n",
    "        return torch.cat((top, bottom), 0)\n",
    "\n",
    "    phi = phi1 if r == 1 else phi2\n",
    "    return phi\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRAIN (from train.py)\n",
    "# ------------------------------\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def example_1(ts: torch.Tensor, epochs: int = 1000):\n",
    "    D = torch.tensor([-9.54, -8.16, -4.26, -11.43], dtype=torch.float32, device=device)\n",
    "    A = torch.tensor([[3.18, 2.72, 1.42, 3.81]], dtype=torch.float32, device=device)\n",
    "    b = torch.tensor([[7.81]], dtype=torch.float32, device=device)\n",
    "    phi = createPhi(D, A, b)\n",
    "    out_vars = sum(A.shape)\n",
    "    model = train_model(phi, ts, device, lr=0.001, epochs=epochs, out_vars=out_vars)\n",
    "    test_times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    test_model(model, test_times, device=device)\n",
    "    save_model(model, f\"example_1_pinn_{epochs}\")\n",
    "\n",
    "\n",
    "def example_2(ts: torch.Tensor, epochs: int = 1000):\n",
    "    D = torch.tensor([-3, -1, -3], dtype=torch.float32, device=device)\n",
    "    A = torch.tensor(\n",
    "        [[2, 1, 1], [1, 2, 3], [2, 2, 1], [-1, 0, 0], [0, -1, 0], [0, 0, -1]],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    b = torch.tensor([2, 5, 6, 0, 0, 0], dtype=torch.float32, device=device)\n",
    "    phi = createPhi(D, A, b)\n",
    "    out_vars = sum(A.shape)\n",
    "    model = train_model(phi, ts, device, lr=0.001, epochs=epochs, out_vars=out_vars)\n",
    "    test_times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    test_model(model, test_times, device=device)\n",
    "    save_model(model, f\"example_2_pinn_{epochs}\")\n",
    "\n",
    "\n",
    "def example_3(ts: torch.Tensor, epochs: int = 1000):\n",
    "    D = torch.tensor([-1.0, -4.0, -3.0], dtype=torch.float32, device=device)\n",
    "    A = torch.tensor(\n",
    "        [\n",
    "            [2.0, 2.0, 1.0],\n",
    "            [1.0, 2.0, 2.0],\n",
    "            [-1.0, 0.0, 0.0],\n",
    "            [0.0, -1.0, 0.0],\n",
    "            [0.0, 0.0, -1.0],\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "    b = torch.tensor([4.0, 6.0, 0.0, 0.0, 0.0], dtype=torch.float32, device=device)\n",
    "    phi = createPhi(D, A, b)\n",
    "    out_vars = sum(A.shape)\n",
    "    model = train_model(phi, ts, device, lr=0.001, epochs=epochs, out_vars=out_vars)\n",
    "    test_times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "    test_model(model, test_times, device=device)\n",
    "    save_model(model, f\"example_3_pinn_{epochs}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN: Set up collocation points and run examples\n",
    "# ------------------------------\n",
    "ts = torch.linspace(0, 10, 128, dtype=torch.float32, device=device)\n",
    "example_1(ts, epochs=1000)\n",
    "example_2(ts, epochs=1000)\n",
    "example_3(ts, epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
