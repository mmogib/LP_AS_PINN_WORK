{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.functional import jacobian\n",
    "from functorch import vmap, jacrev\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Set up the device: GPU if available, otherwise CPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Define ODE constants and the ODE function φ on the selected device\n",
    "# ------------------------------\n",
    "D = torch.tensor([-9.54, -8.16, -4.26, -11.43], dtype=torch.float32, device=device)\n",
    "A = torch.tensor(\n",
    "    [3.18, 2.72, 1.42, 3.81], dtype=torch.float32, device=device\n",
    ")  # shape: (4,)\n",
    "b = 7.81  # scalar\n",
    "\n",
    "\n",
    "def phi(y):\n",
    "    \"\"\"\n",
    "    Given y (a tensor of shape (5,)), compute:\n",
    "      x = y[:4]\n",
    "      u = y[4]\n",
    "      m = max(0, u + dot(A, x) - b)\n",
    "    and return\n",
    "      [ -(D + A*m); m - u ] as a tensor of shape (5,)\n",
    "    \"\"\"\n",
    "    x = y[:4]\n",
    "    u = y[4]\n",
    "    m = torch.clamp(u + torch.dot(A, x) - b, min=0.0)\n",
    "    top = -(D + A * m)\n",
    "    bottom = m - u\n",
    "    return torch.cat((top, bottom.unsqueeze(0)))  # resulting shape: (5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.1976, -3.5904, -1.8744, -5.0292,  3.3200], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.ones((5,), device=device).squeeze(0)\n",
    "phi(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Define the PINN model (moved to GPU)\n",
    "# ------------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        # Two-layer network: input dimension 1 -> 100 -> 5\n",
    "        self.fc1 = nn.Linear(1, 100)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Forward pass for a scalar (or batch) time input.\n",
    "        We assume t is a tensor of shape (N, 1) or a scalar tensor.\n",
    "        The network output is modulated as:\n",
    "            ŷ(t) = (1 - exp(-t)) * NN(t)\n",
    "        to enforce ŷ(0) = 0.\n",
    "        \"\"\"\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        x = self.activation(self.fc1(t))\n",
    "        out = self.fc2(x)\n",
    "        return (1 - torch.exp(-t)) * out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and move the model to the device\n",
    "model = PINN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Set up collocation points on the device\n",
    "# ------------------------------\n",
    "# 100 points uniformly in [0,10]\n",
    "ts = torch.linspace(0, 10, 100, dtype=torch.float32, device=device)  # shape (100,)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Define the vectorized loss function using functorch\n",
    "# ------------------------------\n",
    "def compute_loss_vectorized():\n",
    "    # ts: shape (N,) ; we need to work with scalar inputs, so we keep ts as a 1D tensor.\n",
    "    # Evaluate the PINN on all collocation points.\n",
    "    # The model expects input shape (N,1), so unsqueeze ts.\n",
    "    ts_var = ts.clone().detach().requires_grad_(True)  # shape (N,)\n",
    "    y_hat = model(ts_var.unsqueeze(1))  # shape: (N, 5)\n",
    "\n",
    "    # Define a function that maps a scalar t to the model output (a vector of shape (5,))\n",
    "    def model_single(t):\n",
    "        # t is a scalar; model expects shape (1,1)\n",
    "        return model(t.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    # Compute the derivative dy/dt for each scalar time t using vectorized jacobian.\n",
    "    # jacrev computes the Jacobian of model_single at a scalar t (output shape: (5,))\n",
    "    dy_dt = torch.vmap(torch.func.jacrev(model_single))(ts_var)  # shape: (N, 5)\n",
    "\n",
    "    # Vectorize phi over the batch dimension.\n",
    "    phi_y = torch.vmap(phi)(y_hat)  # shape: (N, 5)\n",
    "\n",
    "    # Compute the residuals at each collocation point.\n",
    "    residuals = dy_dt - phi_y  # shape: (N, 5)\n",
    "    # Compute the mean squared residual over the collocation points.\n",
    "    loss = torch.mean(torch.sum(residuals**2, dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(305.8589, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_vectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 10: Loss = 11.310784\n",
      "Epoch 20: Loss = 0.307400\n",
      "Epoch 30: Loss = 0.553700\n",
      "Epoch 40: Loss = 0.542785\n",
      "Epoch 50: Loss = 0.223981\n",
      "Epoch 60: Loss = 0.082145\n",
      "Epoch 70: Loss = 0.027667\n",
      "Epoch 80: Loss = 0.005278\n",
      "Epoch 90: Loss = 0.000062\n",
      "Epoch 100: Loss = 0.001022\n",
      "Epoch 110: Loss = 0.000412\n",
      "Epoch 120: Loss = 0.000087\n",
      "Epoch 130: Loss = 0.000117\n",
      "Epoch 140: Loss = 0.000067\n",
      "Epoch 150: Loss = 0.000064\n",
      "Epoch 160: Loss = 0.000063\n",
      "Epoch 170: Loss = 0.000061\n",
      "Epoch 180: Loss = 0.000061\n",
      "Epoch 190: Loss = 0.000061\n",
      "Epoch 200: Loss = 0.000061\n",
      "Epoch 210: Loss = 0.000060\n",
      "Epoch 220: Loss = 0.000060\n",
      "Epoch 230: Loss = 0.000060\n",
      "Epoch 240: Loss = 0.000060\n",
      "Epoch 250: Loss = 0.000060\n",
      "Epoch 260: Loss = 0.000060\n",
      "Epoch 270: Loss = 0.000060\n",
      "Epoch 280: Loss = 0.000060\n",
      "Epoch 290: Loss = 0.000060\n",
      "Epoch 300: Loss = 0.000060\n",
      "Epoch 310: Loss = 0.000060\n",
      "Epoch 320: Loss = 0.000060\n",
      "Epoch 330: Loss = 0.000060\n",
      "Epoch 340: Loss = 0.000060\n",
      "Epoch 350: Loss = 0.000060\n",
      "Epoch 360: Loss = 0.000060\n",
      "Epoch 370: Loss = 0.000060\n",
      "Epoch 380: Loss = 0.000060\n",
      "Epoch 390: Loss = 0.000060\n",
      "Epoch 400: Loss = 0.000060\n",
      "Epoch 410: Loss = 0.000060\n",
      "Epoch 420: Loss = 0.000060\n",
      "Epoch 430: Loss = 0.000060\n",
      "Epoch 440: Loss = 0.000060\n",
      "Epoch 450: Loss = 0.000060\n",
      "Epoch 460: Loss = 0.000060\n",
      "Epoch 470: Loss = 0.000060\n",
      "Epoch 480: Loss = 0.000060\n",
      "Epoch 490: Loss = 0.000060\n",
      "Epoch 500: Loss = 0.000060\n",
      "Epoch 510: Loss = 0.000060\n",
      "Epoch 520: Loss = 0.000060\n",
      "Epoch 530: Loss = 0.000060\n",
      "Epoch 540: Loss = 0.000060\n",
      "Epoch 550: Loss = 0.000060\n",
      "Epoch 560: Loss = 0.000060\n",
      "Epoch 570: Loss = 0.000060\n",
      "Epoch 580: Loss = 0.000060\n",
      "Epoch 590: Loss = 0.000060\n",
      "Epoch 600: Loss = 0.000060\n",
      "Epoch 610: Loss = 0.000060\n",
      "Epoch 620: Loss = 0.000060\n",
      "Epoch 630: Loss = 0.000060\n",
      "Epoch 640: Loss = 0.000060\n",
      "Epoch 650: Loss = 0.000060\n",
      "Epoch 660: Loss = 0.000060\n",
      "Epoch 670: Loss = 0.000060\n",
      "Epoch 680: Loss = 0.000060\n",
      "Epoch 690: Loss = 0.000060\n",
      "Epoch 700: Loss = 0.000060\n",
      "Epoch 710: Loss = 0.000060\n",
      "Epoch 720: Loss = 0.000060\n",
      "Epoch 730: Loss = 0.000060\n",
      "Epoch 740: Loss = 0.000060\n",
      "Epoch 750: Loss = 0.000060\n",
      "Epoch 760: Loss = 0.000060\n",
      "Epoch 770: Loss = 0.000060\n",
      "Epoch 780: Loss = 0.000060\n",
      "Epoch 790: Loss = 0.000060\n",
      "Epoch 800: Loss = 0.000060\n",
      "Epoch 810: Loss = 0.000060\n",
      "Epoch 820: Loss = 0.000060\n",
      "Epoch 830: Loss = 0.000060\n",
      "Epoch 840: Loss = 0.000060\n",
      "Epoch 850: Loss = 0.000060\n",
      "Epoch 860: Loss = 0.000060\n",
      "Epoch 870: Loss = 0.000060\n",
      "Epoch 880: Loss = 0.000060\n",
      "Epoch 890: Loss = 0.000060\n",
      "Epoch 900: Loss = 0.000060\n",
      "Epoch 910: Loss = 0.000060\n",
      "Epoch 920: Loss = 0.000060\n",
      "Epoch 930: Loss = 0.000060\n",
      "Epoch 940: Loss = 0.000060\n",
      "Epoch 950: Loss = 0.000060\n",
      "Epoch 960: Loss = 0.000060\n",
      "Epoch 970: Loss = 0.000060\n",
      "Epoch 980: Loss = 0.000060\n",
      "Epoch 990: Loss = 0.000060\n",
      "Epoch 1000: Loss = 0.000060\n",
      "Epoch 1010: Loss = 0.000060\n",
      "Epoch 1020: Loss = 0.000060\n",
      "Epoch 1030: Loss = 0.000060\n",
      "Epoch 1040: Loss = 0.000060\n",
      "Epoch 1050: Loss = 0.000060\n",
      "Epoch 1060: Loss = 0.000060\n",
      "Epoch 1070: Loss = 0.000060\n",
      "Epoch 1080: Loss = 0.000060\n",
      "Epoch 1090: Loss = 0.000060\n",
      "Epoch 1100: Loss = 0.000060\n",
      "Epoch 1110: Loss = 0.000060\n",
      "Epoch 1120: Loss = 0.000060\n",
      "Epoch 1130: Loss = 0.000060\n",
      "Epoch 1140: Loss = 0.000060\n",
      "Epoch 1150: Loss = 0.000060\n",
      "Epoch 1160: Loss = 0.000060\n",
      "Epoch 1170: Loss = 0.000060\n",
      "Epoch 1180: Loss = 0.000060\n",
      "Epoch 1190: Loss = 0.000060\n",
      "Epoch 1200: Loss = 0.000060\n",
      "Epoch 1210: Loss = 0.000060\n",
      "Epoch 1220: Loss = 0.000060\n",
      "Epoch 1230: Loss = 0.000060\n",
      "Epoch 1240: Loss = 0.000059\n",
      "Epoch 1250: Loss = 0.000059\n",
      "Epoch 1260: Loss = 0.000059\n",
      "Epoch 1270: Loss = 0.000059\n",
      "Epoch 1280: Loss = 0.000059\n",
      "Epoch 1290: Loss = 0.000059\n",
      "Epoch 1300: Loss = 0.000059\n",
      "Epoch 1310: Loss = 0.000059\n",
      "Epoch 1320: Loss = 0.000059\n",
      "Epoch 1330: Loss = 0.000059\n",
      "Epoch 1340: Loss = 0.000059\n",
      "Epoch 1350: Loss = 0.000059\n",
      "Epoch 1360: Loss = 0.000059\n",
      "Epoch 1370: Loss = 0.000059\n",
      "Epoch 1380: Loss = 0.000059\n",
      "Epoch 1390: Loss = 0.000059\n",
      "Epoch 1400: Loss = 0.000059\n",
      "Epoch 1410: Loss = 0.000059\n",
      "Epoch 1420: Loss = 0.000059\n",
      "Epoch 1430: Loss = 0.000059\n",
      "Epoch 1440: Loss = 0.000059\n",
      "Epoch 1450: Loss = 0.000059\n",
      "Epoch 1460: Loss = 0.000059\n",
      "Epoch 1470: Loss = 0.000059\n",
      "Epoch 1480: Loss = 0.000059\n",
      "Epoch 1490: Loss = 0.000059\n",
      "Epoch 1500: Loss = 0.000059\n",
      "Epoch 1510: Loss = 0.000059\n",
      "Epoch 1520: Loss = 0.000059\n",
      "Epoch 1530: Loss = 0.000059\n",
      "Epoch 1540: Loss = 0.000059\n",
      "Epoch 1550: Loss = 0.000059\n",
      "Epoch 1560: Loss = 0.000059\n",
      "Epoch 1570: Loss = 0.000059\n",
      "Epoch 1580: Loss = 0.000059\n",
      "Epoch 1590: Loss = 0.000059\n",
      "Epoch 1600: Loss = 0.000059\n",
      "Epoch 1610: Loss = 0.000059\n",
      "Epoch 1620: Loss = 0.000059\n",
      "Epoch 1630: Loss = 0.000059\n",
      "Epoch 1640: Loss = 0.000059\n",
      "Epoch 1650: Loss = 0.000059\n",
      "Epoch 1660: Loss = 0.000059\n",
      "Epoch 1670: Loss = 0.000059\n",
      "Epoch 1680: Loss = 0.000059\n",
      "Epoch 1690: Loss = 0.000059\n",
      "Epoch 1700: Loss = 0.000059\n",
      "Epoch 1710: Loss = 0.000059\n",
      "Epoch 1720: Loss = 0.000059\n",
      "Epoch 1730: Loss = 0.000059\n",
      "Epoch 1740: Loss = 0.000059\n",
      "Epoch 1750: Loss = 0.000059\n",
      "Epoch 1760: Loss = 0.000059\n",
      "Epoch 1770: Loss = 0.000059\n",
      "Epoch 1780: Loss = 0.000059\n",
      "Epoch 1790: Loss = 0.000059\n",
      "Epoch 1800: Loss = 0.000059\n",
      "Epoch 1810: Loss = 0.000059\n",
      "Epoch 1820: Loss = 0.000059\n",
      "Epoch 1830: Loss = 0.000059\n",
      "Epoch 1840: Loss = 0.000059\n",
      "Epoch 1850: Loss = 0.000059\n",
      "Epoch 1860: Loss = 0.000059\n",
      "Epoch 1870: Loss = 0.000059\n",
      "Epoch 1880: Loss = 0.000059\n",
      "Epoch 1890: Loss = 0.000059\n",
      "Epoch 1900: Loss = 0.000059\n",
      "Epoch 1910: Loss = 0.000059\n",
      "Epoch 1920: Loss = 0.000059\n",
      "Epoch 1930: Loss = 0.000059\n",
      "Epoch 1940: Loss = 0.000059\n",
      "Epoch 1950: Loss = 0.000059\n",
      "Epoch 1960: Loss = 0.000059\n",
      "Epoch 1970: Loss = 0.000059\n",
      "Epoch 1980: Loss = 0.000059\n",
      "Epoch 1990: Loss = 0.000059\n",
      "Epoch 2000: Loss = 0.000067\n",
      "Epoch 2010: Loss = 0.000735\n",
      "Epoch 2020: Loss = 0.121859\n",
      "Epoch 2030: Loss = 0.055996\n",
      "Epoch 2040: Loss = 0.009499\n",
      "Epoch 2050: Loss = 0.007359\n",
      "Epoch 2060: Loss = 0.004252\n",
      "Epoch 2070: Loss = 0.003141\n",
      "Epoch 2080: Loss = 0.002322\n",
      "Epoch 2090: Loss = 0.001228\n",
      "Epoch 2100: Loss = 0.000306\n",
      "Epoch 2110: Loss = 0.000060\n",
      "Epoch 2120: Loss = 0.000109\n",
      "Epoch 2130: Loss = 0.000064\n",
      "Epoch 2140: Loss = 0.000065\n",
      "Epoch 2150: Loss = 0.000059\n",
      "Epoch 2160: Loss = 0.000060\n",
      "Epoch 2170: Loss = 0.000059\n",
      "Epoch 2180: Loss = 0.000059\n",
      "Epoch 2190: Loss = 0.000059\n",
      "Epoch 2200: Loss = 0.000059\n",
      "Epoch 2210: Loss = 0.000059\n",
      "Epoch 2220: Loss = 0.000059\n",
      "Epoch 2230: Loss = 0.000059\n",
      "Epoch 2240: Loss = 0.000059\n",
      "Epoch 2250: Loss = 0.000060\n",
      "Epoch 2260: Loss = 0.000086\n",
      "Epoch 2270: Loss = 0.001822\n",
      "Epoch 2280: Loss = 0.222458\n",
      "Epoch 2290: Loss = 0.207362\n",
      "Epoch 2300: Loss = 0.002374\n",
      "Epoch 2310: Loss = 0.021846\n",
      "Epoch 2320: Loss = 0.017886\n",
      "Epoch 2330: Loss = 0.005959\n",
      "Epoch 2340: Loss = 0.000231\n",
      "Epoch 2350: Loss = 0.000560\n",
      "Epoch 2360: Loss = 0.000303\n",
      "Epoch 2370: Loss = 0.000087\n",
      "Epoch 2380: Loss = 0.000086\n",
      "Epoch 2390: Loss = 0.000072\n",
      "Epoch 2400: Loss = 0.000059\n",
      "Epoch 2410: Loss = 0.000059\n",
      "Epoch 2420: Loss = 0.000059\n",
      "Epoch 2430: Loss = 0.000059\n",
      "Epoch 2440: Loss = 0.000059\n",
      "Epoch 2450: Loss = 0.000059\n",
      "Epoch 2460: Loss = 0.000059\n",
      "Epoch 2470: Loss = 0.000059\n",
      "Epoch 2480: Loss = 0.000059\n",
      "Epoch 2490: Loss = 0.000059\n",
      "Epoch 2500: Loss = 0.000086\n",
      "Epoch 2510: Loss = 0.002308\n",
      "Epoch 2520: Loss = 0.324699\n",
      "Epoch 2530: Loss = 0.390329\n",
      "Epoch 2540: Loss = 0.036151\n",
      "Epoch 2550: Loss = 0.000689\n",
      "Epoch 2560: Loss = 0.001914\n",
      "Epoch 2570: Loss = 0.004088\n",
      "Epoch 2580: Loss = 0.002593\n",
      "Epoch 2590: Loss = 0.000412\n",
      "Epoch 2600: Loss = 0.000116\n",
      "Epoch 2610: Loss = 0.000178\n",
      "Epoch 2620: Loss = 0.000059\n",
      "Epoch 2630: Loss = 0.000073\n",
      "Epoch 2640: Loss = 0.000061\n",
      "Epoch 2650: Loss = 0.000059\n",
      "Epoch 2660: Loss = 0.000059\n",
      "Epoch 2670: Loss = 0.000059\n",
      "Epoch 2680: Loss = 0.000059\n",
      "Epoch 2690: Loss = 0.000058\n",
      "Epoch 2700: Loss = 0.000058\n",
      "Epoch 2710: Loss = 0.000058\n",
      "Epoch 2720: Loss = 0.000058\n",
      "Epoch 2730: Loss = 0.000058\n",
      "Epoch 2740: Loss = 0.000058\n",
      "Epoch 2750: Loss = 0.000058\n",
      "Epoch 2760: Loss = 0.000060\n",
      "Epoch 2770: Loss = 0.000246\n",
      "Epoch 2780: Loss = 0.051402\n",
      "Epoch 2790: Loss = 0.137702\n",
      "Epoch 2800: Loss = 0.098553\n",
      "Epoch 2810: Loss = 0.004519\n",
      "Epoch 2820: Loss = 0.002878\n",
      "Epoch 2830: Loss = 0.005526\n",
      "Epoch 2840: Loss = 0.003378\n",
      "Epoch 2850: Loss = 0.001460\n",
      "Epoch 2860: Loss = 0.000555\n",
      "Epoch 2870: Loss = 0.000204\n",
      "Epoch 2880: Loss = 0.000084\n",
      "Epoch 2890: Loss = 0.000059\n",
      "Epoch 2900: Loss = 0.000062\n",
      "Epoch 2910: Loss = 0.000062\n",
      "Epoch 2920: Loss = 0.000059\n",
      "Epoch 2930: Loss = 0.000059\n",
      "Epoch 2940: Loss = 0.000058\n",
      "Epoch 2950: Loss = 0.000058\n",
      "Epoch 2960: Loss = 0.000058\n",
      "Epoch 2970: Loss = 0.000058\n",
      "Epoch 2980: Loss = 0.000058\n",
      "Epoch 2990: Loss = 0.000058\n",
      "Epoch 3000: Loss = 0.000058\n",
      "Epoch 3010: Loss = 0.000058\n",
      "Epoch 3020: Loss = 0.000058\n",
      "Epoch 3030: Loss = 0.000058\n",
      "Epoch 3040: Loss = 0.000058\n",
      "Epoch 3050: Loss = 0.000058\n",
      "Epoch 3060: Loss = 0.000059\n",
      "Epoch 3070: Loss = 0.000065\n",
      "Epoch 3080: Loss = 0.000378\n",
      "Epoch 3090: Loss = 0.033106\n",
      "Epoch 3100: Loss = 0.676046\n",
      "Epoch 3110: Loss = 0.105522\n",
      "Epoch 3120: Loss = 0.000270\n",
      "Epoch 3130: Loss = 0.008305\n",
      "Epoch 3140: Loss = 0.009823\n",
      "Epoch 3150: Loss = 0.003073\n",
      "Epoch 3160: Loss = 0.000061\n",
      "Epoch 3170: Loss = 0.000499\n",
      "Epoch 3180: Loss = 0.000110\n",
      "Epoch 3190: Loss = 0.000112\n",
      "Epoch 3200: Loss = 0.000059\n",
      "Epoch 3210: Loss = 0.000067\n",
      "Epoch 3220: Loss = 0.000061\n",
      "Epoch 3230: Loss = 0.000059\n",
      "Epoch 3240: Loss = 0.000058\n",
      "Epoch 3250: Loss = 0.000058\n",
      "Epoch 3260: Loss = 0.000058\n",
      "Epoch 3270: Loss = 0.000058\n",
      "Epoch 3280: Loss = 0.000058\n",
      "Epoch 3290: Loss = 0.000058\n",
      "Epoch 3300: Loss = 0.000058\n",
      "Epoch 3310: Loss = 0.000059\n",
      "Epoch 3320: Loss = 0.000072\n",
      "Epoch 3330: Loss = 0.000977\n",
      "Epoch 3340: Loss = 0.124318\n",
      "Epoch 3350: Loss = 0.005814\n",
      "Epoch 3360: Loss = 0.097159\n",
      "Epoch 3370: Loss = 0.062160\n",
      "Epoch 3380: Loss = 0.024032\n",
      "Epoch 3390: Loss = 0.005098\n",
      "Epoch 3400: Loss = 0.000117\n",
      "Epoch 3410: Loss = 0.000598\n",
      "Epoch 3420: Loss = 0.000443\n",
      "Epoch 3430: Loss = 0.000058\n",
      "Epoch 3440: Loss = 0.000112\n",
      "Epoch 3450: Loss = 0.000058\n",
      "Epoch 3460: Loss = 0.000064\n",
      "Epoch 3470: Loss = 0.000060\n",
      "Epoch 3480: Loss = 0.000058\n",
      "Epoch 3490: Loss = 0.000058\n",
      "Epoch 3500: Loss = 0.000058\n",
      "Epoch 3510: Loss = 0.000058\n",
      "Epoch 3520: Loss = 0.000058\n",
      "Epoch 3530: Loss = 0.000058\n",
      "Epoch 3540: Loss = 0.000058\n",
      "Epoch 3550: Loss = 0.000058\n",
      "Epoch 3560: Loss = 0.000058\n",
      "Epoch 3570: Loss = 0.000058\n",
      "Epoch 3580: Loss = 0.000058\n",
      "Epoch 3590: Loss = 0.000070\n",
      "Epoch 3600: Loss = 0.001940\n",
      "Epoch 3610: Loss = 0.487635\n",
      "Epoch 3620: Loss = 0.555377\n",
      "Epoch 3630: Loss = 0.186226\n",
      "Epoch 3640: Loss = 0.051691\n",
      "Epoch 3650: Loss = 0.011285\n",
      "Epoch 3660: Loss = 0.002338\n",
      "Epoch 3670: Loss = 0.000679\n",
      "Epoch 3680: Loss = 0.000351\n",
      "Epoch 3690: Loss = 0.000253\n",
      "Epoch 3700: Loss = 0.000179\n",
      "Epoch 3710: Loss = 0.000108\n",
      "Epoch 3720: Loss = 0.000065\n",
      "Epoch 3730: Loss = 0.000058\n",
      "Epoch 3740: Loss = 0.000060\n",
      "Epoch 3750: Loss = 0.000058\n",
      "Epoch 3760: Loss = 0.000058\n",
      "Epoch 3770: Loss = 0.000058\n",
      "Epoch 3780: Loss = 0.000058\n",
      "Epoch 3790: Loss = 0.000058\n",
      "Epoch 3800: Loss = 0.000058\n",
      "Epoch 3810: Loss = 0.000058\n",
      "Epoch 3820: Loss = 0.000058\n",
      "Epoch 3830: Loss = 0.000058\n",
      "Epoch 3840: Loss = 0.000058\n",
      "Epoch 3850: Loss = 0.000058\n",
      "Epoch 3860: Loss = 0.000058\n",
      "Epoch 3870: Loss = 0.000058\n",
      "Epoch 3880: Loss = 0.000058\n",
      "Epoch 3890: Loss = 0.000058\n",
      "Epoch 3900: Loss = 0.000063\n",
      "Epoch 3910: Loss = 0.000504\n",
      "Epoch 3920: Loss = 0.074879\n",
      "Epoch 3930: Loss = 0.069180\n",
      "Epoch 3940: Loss = 0.177113\n",
      "Epoch 3950: Loss = 0.075107\n",
      "Epoch 3960: Loss = 0.029055\n",
      "Epoch 3970: Loss = 0.010863\n",
      "Epoch 3980: Loss = 0.003348\n",
      "Epoch 3990: Loss = 0.000553\n",
      "Epoch 4000: Loss = 0.000060\n",
      "Epoch 4010: Loss = 0.000178\n",
      "Epoch 4020: Loss = 0.000111\n",
      "Epoch 4030: Loss = 0.000058\n",
      "Epoch 4040: Loss = 0.000067\n",
      "Epoch 4050: Loss = 0.000058\n",
      "Epoch 4060: Loss = 0.000059\n",
      "Epoch 4070: Loss = 0.000058\n",
      "Epoch 4080: Loss = 0.000058\n",
      "Epoch 4090: Loss = 0.000058\n",
      "Epoch 4100: Loss = 0.000058\n",
      "Epoch 4110: Loss = 0.000058\n",
      "Epoch 4120: Loss = 0.000058\n",
      "Epoch 4130: Loss = 0.000058\n",
      "Epoch 4140: Loss = 0.000058\n",
      "Epoch 4150: Loss = 0.000058\n",
      "Epoch 4160: Loss = 0.000058\n",
      "Epoch 4170: Loss = 0.000060\n",
      "Epoch 4180: Loss = 0.000113\n",
      "Epoch 4190: Loss = 0.003495\n",
      "Epoch 4200: Loss = 0.360695\n",
      "Epoch 4210: Loss = 0.331087\n",
      "Epoch 4220: Loss = 0.001742\n",
      "Epoch 4230: Loss = 0.018594\n",
      "Epoch 4240: Loss = 0.017452\n",
      "Epoch 4250: Loss = 0.002880\n",
      "Epoch 4260: Loss = 0.000371\n",
      "Epoch 4270: Loss = 0.000883\n",
      "Epoch 4280: Loss = 0.000058\n",
      "Epoch 4290: Loss = 0.000166\n",
      "Epoch 4300: Loss = 0.000068\n",
      "Epoch 4310: Loss = 0.000060\n",
      "Epoch 4320: Loss = 0.000062\n",
      "Epoch 4330: Loss = 0.000060\n",
      "Epoch 4340: Loss = 0.000058\n",
      "Epoch 4350: Loss = 0.000058\n",
      "Epoch 4360: Loss = 0.000058\n",
      "Epoch 4370: Loss = 0.000058\n",
      "Epoch 4380: Loss = 0.000058\n",
      "Epoch 4390: Loss = 0.000058\n",
      "Epoch 4400: Loss = 0.000058\n",
      "Epoch 4410: Loss = 0.000060\n",
      "Epoch 4420: Loss = 0.000130\n",
      "Epoch 4430: Loss = 0.004839\n",
      "Epoch 4440: Loss = 0.483514\n",
      "Epoch 4450: Loss = 0.378364\n",
      "Epoch 4460: Loss = 0.032653\n",
      "Epoch 4470: Loss = 0.002489\n",
      "Epoch 4480: Loss = 0.012231\n",
      "Epoch 4490: Loss = 0.005043\n",
      "Epoch 4500: Loss = 0.000064\n",
      "Epoch 4510: Loss = 0.000754\n",
      "Epoch 4520: Loss = 0.000100\n",
      "Epoch 4530: Loss = 0.000154\n",
      "Epoch 4540: Loss = 0.000058\n",
      "Epoch 4550: Loss = 0.000066\n",
      "Epoch 4560: Loss = 0.000063\n",
      "Epoch 4570: Loss = 0.000059\n",
      "Epoch 4580: Loss = 0.000058\n",
      "Epoch 4590: Loss = 0.000058\n",
      "Epoch 4600: Loss = 0.000058\n",
      "Epoch 4610: Loss = 0.000057\n",
      "Epoch 4620: Loss = 0.000057\n",
      "Epoch 4630: Loss = 0.000057\n",
      "Epoch 4640: Loss = 0.000057\n",
      "Epoch 4650: Loss = 0.000057\n",
      "Epoch 4660: Loss = 0.000059\n",
      "Epoch 4670: Loss = 0.000170\n",
      "Epoch 4680: Loss = 0.015255\n",
      "Epoch 4690: Loss = 1.120449\n",
      "Epoch 4700: Loss = 0.030059\n",
      "Epoch 4710: Loss = 0.000086\n",
      "Epoch 4720: Loss = 0.001634\n",
      "Epoch 4730: Loss = 0.002677\n",
      "Epoch 4740: Loss = 0.002769\n",
      "Epoch 4750: Loss = 0.001839\n",
      "Epoch 4760: Loss = 0.000621\n",
      "Epoch 4770: Loss = 0.000076\n",
      "Epoch 4780: Loss = 0.000101\n",
      "Epoch 4790: Loss = 0.000085\n",
      "Epoch 4800: Loss = 0.000058\n",
      "Epoch 4810: Loss = 0.000061\n",
      "Epoch 4820: Loss = 0.000058\n",
      "Epoch 4830: Loss = 0.000057\n",
      "Epoch 4840: Loss = 0.000058\n",
      "Epoch 4850: Loss = 0.000057\n",
      "Epoch 4860: Loss = 0.000057\n",
      "Epoch 4870: Loss = 0.000057\n",
      "Epoch 4880: Loss = 0.000057\n",
      "Epoch 4890: Loss = 0.000057\n",
      "Epoch 4900: Loss = 0.000057\n",
      "Epoch 4910: Loss = 0.000057\n",
      "Epoch 4920: Loss = 0.000057\n",
      "Epoch 4930: Loss = 0.000057\n",
      "Epoch 4940: Loss = 0.000057\n",
      "Epoch 4950: Loss = 0.000070\n",
      "Epoch 4960: Loss = 0.001411\n",
      "Epoch 4970: Loss = 0.258910\n",
      "Epoch 4980: Loss = 0.383083\n",
      "Epoch 4990: Loss = 0.043479\n",
      "Epoch 5000: Loss = 0.009993\n",
      "Training complete.\n",
      "\n",
      "t =  0.0, ŷ(t) = [ 0.  0.  0.  0. -0.]\n",
      "t =  2.5, ŷ(t) = [0.7500235  0.6408357  0.33284432 0.9037617  2.7404554 ]\n",
      "t =  5.0, ŷ(t) = [0.7355519  0.6231557  0.32060164 0.8671471  2.9786491 ]\n",
      "t =  7.5, ŷ(t) = [0.7280151  0.622691   0.31920895 0.86897695 2.9967997 ]\n",
      "t = 10.0, ŷ(t) = [0.726096   0.621167   0.3206678  0.87122124 2.9952447 ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Training Loop using Adam (lr = 0.001) on the GPU\n",
    "# ------------------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5000\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    loss_val = compute_loss_vectorized()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs.\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss_val.item():.6f}\")\n",
    "\n",
    "    # Stop if loss is below the threshold.\n",
    "    if loss_val.item() < 1e-6:\n",
    "        print(f\"Stopping training at epoch {epoch} with loss = {loss_val.item():.6f}\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate the trained model at select time points\n",
    "# ------------------------------\n",
    "test_times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "for t in test_times:\n",
    "    t_tensor = torch.tensor(t, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_pred = model(t_tensor)\n",
    "    print(f\"t = {t:4.1f}, ŷ(t) = {y_pred.detach().cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7262, 1.6210, 1.3207, 1.8712, 3.9949], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ = torch.tensor(10.2, dtype=torch.float32, device=device, requires_grad=True)\n",
    "y_ = model(t_)\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Evaluate the trained model at select time points and visualize each solution component.\n",
    "# ------------------------------\n",
    "# Create a fine grid over [0,10]\n",
    "t_plot = torch.linspace(0, 10, 200, dtype=torch.float32, device=device)  # shape: (200,)\n",
    "# Evaluate the model on this grid. (Ensure correct shape by unsqueezing.)\n",
    "y_plot = model(t_plot.unsqueeze(1))  # shape: (200, 5)\n",
    "y_plot = y_plot.detach().cpu().numpy()\n",
    "t_plot_np = t_plot.detach().cpu().numpy()\n",
    "\n",
    "# Create a plot for each component y[0]...y[4]\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(5):\n",
    "    plt.plot(t_plot_np, y_plot[:, i], label=f\"y[{i}]\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"PINN Solution Components over [0,10]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
