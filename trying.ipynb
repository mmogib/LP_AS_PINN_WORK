{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.functional import jacobian\n",
    "from functorch import vmap, jacrev\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Set up the device: GPU if available, otherwise CPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Define ODE constants and the ODE function φ on the selected device\n",
    "# ------------------------------\n",
    "D = torch.tensor([-9.54, -8.16, -4.26, -11.43], dtype=torch.float32, device=device)\n",
    "A = torch.tensor(\n",
    "    [3.18, 2.72, 1.42, 3.81], dtype=torch.float32, device=device\n",
    ")  # shape: (4,)\n",
    "b = 7.81  # scalar\n",
    "\n",
    "\n",
    "def phi(y):\n",
    "    \"\"\"\n",
    "    Given y (a tensor of shape (5,)), compute:\n",
    "      x = y[:4]\n",
    "      u = y[4]\n",
    "      m = max(0, u + dot(A, x) - b)\n",
    "    and return\n",
    "      [ -(D + A*m); m - u ] as a tensor of shape (5,)\n",
    "    \"\"\"\n",
    "    x = y[:4]\n",
    "    u = y[4]\n",
    "    m = torch.clamp(u + torch.dot(A, x) - b, min=0.0)\n",
    "    top = -(D + A * m)\n",
    "    bottom = m - u\n",
    "    return torch.cat((top, bottom.unsqueeze(0)))  # resulting shape: (5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = torch.ones((5,), device=device).squeeze(0)\n",
    "# phi(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Define the PINN model (moved to GPU)\n",
    "# ------------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        # Two-layer network: input dimension 1 -> 100 -> 5\n",
    "        self.fc1 = nn.Linear(1, 100)\n",
    "        self.fc2 = nn.Linear(100, 5)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Forward pass for a scalar (or batch) time input.\n",
    "        We assume t is a tensor of shape (N, 1) or a scalar tensor.\n",
    "        The network output is modulated as:\n",
    "            ŷ(t) = (1 - exp(-t)) * NN(t)\n",
    "        to enforce ŷ(0) = 0.\n",
    "        \"\"\"\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        x = self.activation(self.fc1(t))\n",
    "        out = self.fc2(x)\n",
    "        return (1 - torch.exp(-t)) * out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and move the model to the device\n",
    "model = PINN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Set up collocation points on the device\n",
    "# ------------------------------\n",
    "# 100 points uniformly in [0,10]\n",
    "ts = torch.linspace(0, 10, 100, dtype=torch.float32, device=device)  # shape (100,)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Define the vectorized loss function using functorch\n",
    "# ------------------------------\n",
    "def compute_loss_vectorized():\n",
    "    # ts: shape (N,) ; we need to work with scalar inputs, so we keep ts as a 1D tensor.\n",
    "    # Evaluate the PINN on all collocation points.\n",
    "    # The model expects input shape (N,1), so unsqueeze ts.\n",
    "    ts_var = ts.clone().detach().requires_grad_(True)  # shape (N,)\n",
    "    y_hat = model(ts_var.unsqueeze(1))  # shape: (N, 5)\n",
    "\n",
    "    # Define a function that maps a scalar t to the model output (a vector of shape (5,))\n",
    "    def model_single(t):\n",
    "        # t is a scalar; model expects shape (1,1)\n",
    "        return model(t.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    # Compute the derivative dy/dt for each scalar time t using vectorized jacobian.\n",
    "    # jacrev computes the Jacobian of model_single at a scalar t (output shape: (5,))\n",
    "    dy_dt = torch.vmap(torch.func.jacrev(model_single))(ts_var)  # shape: (N, 5)\n",
    "\n",
    "    # Vectorize phi over the batch dimension.\n",
    "    phi_y = torch.vmap(phi)(y_hat)  # shape: (N, 5)\n",
    "\n",
    "    # Compute the residuals at each collocation point.\n",
    "    residuals = dy_dt - phi_y  # shape: (N, 5)\n",
    "    # Compute the mean squared residual over the collocation points.\n",
    "    loss = torch.mean(torch.sum(residuals**2, dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_loss_vectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 10: Loss = 98.145134\n",
      "Epoch 20: Loss = 92.299706\n",
      "Epoch 30: Loss = 74.305138\n",
      "Epoch 40: Loss = 60.802822\n",
      "Epoch 50: Loss = 53.295628\n",
      "Epoch 60: Loss = 50.553516\n",
      "Epoch 70: Loss = 48.616375\n",
      "Epoch 80: Loss = 46.806499\n",
      "Epoch 90: Loss = 45.250790\n",
      "Epoch 100: Loss = 43.746441\n",
      "Epoch 110: Loss = 42.129402\n",
      "Epoch 120: Loss = 40.522514\n",
      "Epoch 130: Loss = 39.093140\n",
      "Epoch 140: Loss = 37.805561\n",
      "Epoch 150: Loss = 36.335915\n",
      "Epoch 160: Loss = 35.016201\n",
      "Epoch 170: Loss = 33.885036\n",
      "Epoch 180: Loss = 32.888996\n",
      "Epoch 190: Loss = 31.708719\n",
      "Epoch 200: Loss = 30.659184\n",
      "Epoch 210: Loss = 29.773582\n",
      "Epoch 220: Loss = 29.025282\n",
      "Epoch 230: Loss = 28.393898\n",
      "Epoch 240: Loss = 27.548117\n",
      "Epoch 250: Loss = 26.784922\n",
      "Epoch 260: Loss = 26.136253\n",
      "Epoch 270: Loss = 25.577135\n",
      "Epoch 280: Loss = 25.101414\n",
      "Epoch 290: Loss = 24.691139\n",
      "Epoch 300: Loss = 24.309763\n",
      "Epoch 310: Loss = 23.718740\n",
      "Epoch 320: Loss = 23.181992\n",
      "Epoch 330: Loss = 22.695177\n",
      "Epoch 340: Loss = 22.261328\n",
      "Epoch 350: Loss = 21.877590\n",
      "Epoch 360: Loss = 21.533504\n",
      "Epoch 370: Loss = 21.223280\n",
      "Epoch 380: Loss = 20.941481\n",
      "Epoch 390: Loss = 20.684072\n",
      "Epoch 400: Loss = 20.447880\n",
      "Epoch 410: Loss = 20.105156\n",
      "Epoch 420: Loss = 19.683231\n",
      "Epoch 430: Loss = 19.293808\n",
      "Epoch 440: Loss = 18.932529\n",
      "Epoch 450: Loss = 18.608839\n",
      "Epoch 460: Loss = 18.314667\n",
      "Epoch 470: Loss = 18.047058\n",
      "Epoch 480: Loss = 17.802258\n",
      "Epoch 490: Loss = 17.577610\n",
      "Epoch 500: Loss = 17.370863\n",
      "Epoch 510: Loss = 17.180130\n",
      "Epoch 520: Loss = 17.003778\n",
      "Epoch 530: Loss = 16.840372\n",
      "Epoch 540: Loss = 16.688643\n",
      "Epoch 550: Loss = 16.547457\n",
      "Epoch 560: Loss = 16.415806\n",
      "Epoch 570: Loss = 16.249855\n",
      "Epoch 580: Loss = 15.951189\n",
      "Epoch 590: Loss = 15.673482\n",
      "Epoch 600: Loss = 15.413371\n",
      "Epoch 610: Loss = 15.180162\n",
      "Epoch 620: Loss = 14.965609\n",
      "Epoch 630: Loss = 14.767950\n",
      "Epoch 640: Loss = 14.584858\n",
      "Epoch 650: Loss = 14.414939\n",
      "Epoch 660: Loss = 14.256742\n",
      "Epoch 670: Loss = 14.109022\n",
      "Epoch 680: Loss = 13.970657\n",
      "Epoch 690: Loss = 13.840677\n",
      "Epoch 700: Loss = 13.718207\n",
      "Epoch 710: Loss = 13.602497\n",
      "Epoch 720: Loss = 13.492886\n",
      "Epoch 730: Loss = 13.388782\n",
      "Epoch 740: Loss = 13.289662\n",
      "Epoch 750: Loss = 13.195066\n",
      "Epoch 760: Loss = 13.104592\n",
      "Epoch 770: Loss = 13.017883\n",
      "Epoch 780: Loss = 12.934615\n",
      "Epoch 790: Loss = 12.854507\n",
      "Epoch 800: Loss = 12.777322\n",
      "Epoch 810: Loss = 12.702826\n",
      "Epoch 820: Loss = 12.630836\n",
      "Epoch 830: Loss = 12.561174\n",
      "Epoch 840: Loss = 12.493689\n",
      "Epoch 850: Loss = 12.428243\n",
      "Epoch 860: Loss = 12.301958\n",
      "Epoch 870: Loss = 12.100993\n",
      "Epoch 880: Loss = 11.911661\n",
      "Epoch 890: Loss = 11.744012\n",
      "Epoch 900: Loss = 11.585240\n",
      "Epoch 910: Loss = 11.434884\n",
      "Epoch 920: Loss = 11.292356\n",
      "Epoch 930: Loss = 11.157408\n",
      "Epoch 940: Loss = 11.029205\n",
      "Epoch 950: Loss = 10.907089\n",
      "Epoch 960: Loss = 10.790527\n",
      "Epoch 970: Loss = 10.679073\n",
      "Epoch 980: Loss = 10.572322\n",
      "Epoch 990: Loss = 10.469924\n",
      "Epoch 1000: Loss = 10.371579\n",
      "Epoch 1010: Loss = 10.277013\n",
      "Epoch 1020: Loss = 10.185992\n",
      "Epoch 1030: Loss = 10.098303\n",
      "Epoch 1040: Loss = 10.013761\n",
      "Epoch 1050: Loss = 9.932186\n",
      "Epoch 1060: Loss = 9.853428\n",
      "Epoch 1070: Loss = 9.777346\n",
      "Epoch 1080: Loss = 9.703807\n",
      "Epoch 1090: Loss = 9.632692\n",
      "Epoch 1100: Loss = 9.563890\n",
      "Epoch 1110: Loss = 9.497298\n",
      "Epoch 1120: Loss = 9.432814\n",
      "Epoch 1130: Loss = 9.370351\n",
      "Epoch 1140: Loss = 9.309821\n",
      "Epoch 1150: Loss = 9.251145\n",
      "Epoch 1160: Loss = 9.194248\n",
      "Epoch 1170: Loss = 9.139055\n",
      "Epoch 1180: Loss = 9.085498\n",
      "Epoch 1190: Loss = 9.033515\n",
      "Epoch 1200: Loss = 8.983041\n",
      "Epoch 1210: Loss = 8.934018\n",
      "Epoch 1220: Loss = 8.886389\n",
      "Epoch 1230: Loss = 8.840102\n",
      "Epoch 1240: Loss = 8.795108\n",
      "Epoch 1250: Loss = 8.751353\n",
      "Epoch 1260: Loss = 8.708794\n",
      "Epoch 1270: Loss = 8.667382\n",
      "Epoch 1280: Loss = 8.627082\n",
      "Epoch 1290: Loss = 8.587841\n",
      "Epoch 1300: Loss = 8.549633\n",
      "Epoch 1310: Loss = 8.512408\n",
      "Epoch 1320: Loss = 8.476136\n",
      "Epoch 1330: Loss = 8.440781\n",
      "Epoch 1340: Loss = 8.406308\n",
      "Epoch 1350: Loss = 8.372687\n",
      "Epoch 1360: Loss = 8.339882\n",
      "Epoch 1370: Loss = 8.307866\n",
      "Epoch 1380: Loss = 8.276610\n",
      "Epoch 1390: Loss = 8.246087\n",
      "Epoch 1400: Loss = 8.216269\n",
      "Epoch 1410: Loss = 8.187128\n",
      "Epoch 1420: Loss = 8.141376\n",
      "Epoch 1430: Loss = 7.989451\n",
      "Epoch 1440: Loss = 7.862358\n",
      "Epoch 1450: Loss = 7.755875\n",
      "Epoch 1460: Loss = 7.650365\n",
      "Epoch 1470: Loss = 7.550841\n",
      "Epoch 1480: Loss = 7.457549\n",
      "Epoch 1490: Loss = 7.368797\n",
      "Epoch 1500: Loss = 7.284131\n",
      "Epoch 1510: Loss = 7.203204\n",
      "Epoch 1520: Loss = 7.125649\n",
      "Epoch 1530: Loss = 7.051195\n",
      "Epoch 1540: Loss = 6.979617\n",
      "Epoch 1550: Loss = 6.910721\n",
      "Epoch 1560: Loss = 6.844335\n",
      "Epoch 1570: Loss = 6.780304\n",
      "Epoch 1580: Loss = 6.718493\n",
      "Epoch 1590: Loss = 6.658773\n",
      "Epoch 1600: Loss = 6.601028\n",
      "Epoch 1610: Loss = 6.545146\n",
      "Epoch 1620: Loss = 6.491030\n",
      "Epoch 1630: Loss = 6.438581\n",
      "Epoch 1640: Loss = 6.387714\n",
      "Epoch 1650: Loss = 6.338350\n",
      "Epoch 1660: Loss = 6.290403\n",
      "Epoch 1670: Loss = 6.243810\n",
      "Epoch 1680: Loss = 6.198498\n",
      "Epoch 1690: Loss = 6.154408\n",
      "Epoch 1700: Loss = 6.111478\n",
      "Epoch 1710: Loss = 6.069656\n",
      "Epoch 1720: Loss = 6.028888\n",
      "Epoch 1730: Loss = 5.989125\n",
      "Epoch 1740: Loss = 5.950325\n",
      "Epoch 1750: Loss = 5.912439\n",
      "Epoch 1760: Loss = 5.875432\n",
      "Epoch 1770: Loss = 5.839268\n",
      "Epoch 1780: Loss = 5.803908\n",
      "Epoch 1790: Loss = 5.769321\n",
      "Epoch 1800: Loss = 5.735479\n",
      "Epoch 1810: Loss = 5.702346\n",
      "Epoch 1820: Loss = 5.669901\n",
      "Epoch 1830: Loss = 5.638114\n",
      "Epoch 1840: Loss = 5.606965\n",
      "Epoch 1850: Loss = 5.576430\n",
      "Epoch 1860: Loss = 5.546487\n",
      "Epoch 1870: Loss = 5.517117\n",
      "Epoch 1880: Loss = 5.488299\n",
      "Epoch 1890: Loss = 5.460018\n",
      "Epoch 1900: Loss = 5.432253\n",
      "Epoch 1910: Loss = 5.404990\n",
      "Epoch 1920: Loss = 5.378215\n",
      "Epoch 1930: Loss = 5.351911\n",
      "Epoch 1940: Loss = 5.326067\n",
      "Epoch 1950: Loss = 5.300669\n",
      "Epoch 1960: Loss = 5.275704\n",
      "Epoch 1970: Loss = 5.251160\n",
      "Epoch 1980: Loss = 5.227026\n",
      "Epoch 1990: Loss = 5.203293\n",
      "Epoch 2000: Loss = 5.179950\n",
      "Epoch 2010: Loss = 5.156989\n",
      "Epoch 2020: Loss = 5.134393\n",
      "Epoch 2030: Loss = 5.112164\n",
      "Epoch 2040: Loss = 5.090287\n",
      "Epoch 2050: Loss = 5.068754\n",
      "Epoch 2060: Loss = 5.047558\n",
      "Epoch 2070: Loss = 5.026693\n",
      "Epoch 2080: Loss = 5.006149\n",
      "Epoch 2090: Loss = 4.985921\n",
      "Epoch 2100: Loss = 4.966003\n",
      "Epoch 2110: Loss = 4.946383\n",
      "Epoch 2120: Loss = 4.927061\n",
      "Epoch 2130: Loss = 4.908026\n",
      "Epoch 2140: Loss = 4.889275\n",
      "Epoch 2150: Loss = 4.870800\n",
      "Epoch 2160: Loss = 4.852595\n",
      "Epoch 2170: Loss = 4.834657\n",
      "Epoch 2180: Loss = 4.816979\n",
      "Epoch 2190: Loss = 4.799556\n",
      "Epoch 2200: Loss = 4.782382\n",
      "Epoch 2210: Loss = 4.765452\n",
      "Epoch 2220: Loss = 4.748763\n",
      "Epoch 2230: Loss = 4.732306\n",
      "Epoch 2240: Loss = 4.716080\n",
      "Epoch 2250: Loss = 4.700080\n",
      "Epoch 2260: Loss = 4.684300\n",
      "Epoch 2270: Loss = 4.668737\n",
      "Epoch 2280: Loss = 4.653386\n",
      "Epoch 2290: Loss = 4.638243\n",
      "Epoch 2300: Loss = 4.623304\n",
      "Epoch 2310: Loss = 4.608564\n",
      "Epoch 2320: Loss = 4.594020\n",
      "Epoch 2330: Loss = 4.579668\n",
      "Epoch 2340: Loss = 4.565503\n",
      "Epoch 2350: Loss = 4.551524\n",
      "Epoch 2360: Loss = 4.537725\n",
      "Epoch 2370: Loss = 4.524104\n",
      "Epoch 2380: Loss = 4.510656\n",
      "Epoch 2390: Loss = 4.497380\n",
      "Epoch 2400: Loss = 4.484269\n",
      "Epoch 2410: Loss = 4.471321\n",
      "Epoch 2420: Loss = 4.458536\n",
      "Epoch 2430: Loss = 4.445909\n",
      "Epoch 2440: Loss = 4.433434\n",
      "Epoch 2450: Loss = 4.421112\n",
      "Epoch 2460: Loss = 4.408938\n",
      "Epoch 2470: Loss = 4.396910\n",
      "Epoch 2480: Loss = 4.385026\n",
      "Epoch 2490: Loss = 4.373280\n",
      "Epoch 2500: Loss = 4.361673\n",
      "Epoch 2510: Loss = 4.350201\n",
      "Epoch 2520: Loss = 4.338863\n",
      "Epoch 2530: Loss = 4.327652\n",
      "Epoch 2540: Loss = 4.316570\n",
      "Epoch 2550: Loss = 4.305613\n",
      "Epoch 2560: Loss = 4.294779\n",
      "Epoch 2570: Loss = 4.284065\n",
      "Epoch 2580: Loss = 4.273470\n",
      "Epoch 2590: Loss = 4.262992\n",
      "Epoch 2600: Loss = 4.252626\n",
      "Epoch 2610: Loss = 4.242373\n",
      "Epoch 2620: Loss = 4.232230\n",
      "Epoch 2630: Loss = 4.222195\n",
      "Epoch 2640: Loss = 4.212267\n",
      "Epoch 2650: Loss = 4.202442\n",
      "Epoch 2660: Loss = 4.192721\n",
      "Epoch 2670: Loss = 4.183100\n",
      "Epoch 2680: Loss = 4.173578\n",
      "Epoch 2690: Loss = 4.164153\n",
      "Epoch 2700: Loss = 4.154825\n",
      "Epoch 2710: Loss = 4.145666\n",
      "Epoch 2720: Loss = 4.145488\n",
      "Epoch 2730: Loss = 4.130255\n",
      "Epoch 2740: Loss = 4.122032\n",
      "Epoch 2750: Loss = 4.114322\n",
      "Epoch 2760: Loss = 4.102366\n",
      "Epoch 2770: Loss = 4.092865\n",
      "Epoch 2780: Loss = 4.084300\n",
      "Epoch 2790: Loss = 4.075911\n",
      "Epoch 2800: Loss = 4.067610\n",
      "Epoch 2810: Loss = 4.059368\n",
      "Epoch 2820: Loss = 4.051195\n",
      "Epoch 2830: Loss = 4.043111\n",
      "Epoch 2840: Loss = 3.998807\n",
      "Epoch 2850: Loss = 3.927253\n",
      "Epoch 2860: Loss = 3.838534\n",
      "Epoch 2870: Loss = 3.764519\n",
      "Epoch 2880: Loss = 3.707726\n",
      "Epoch 2890: Loss = 3.655339\n",
      "Epoch 2900: Loss = 3.603535\n",
      "Epoch 2910: Loss = 3.554943\n",
      "Epoch 2920: Loss = 3.508839\n",
      "Epoch 2930: Loss = 3.480902\n",
      "Epoch 2940: Loss = 3.556455\n",
      "Epoch 2950: Loss = 3.402332\n",
      "Epoch 2960: Loss = 3.339550\n",
      "Epoch 2970: Loss = 3.302655\n",
      "Epoch 2980: Loss = 3.264985\n",
      "Epoch 2990: Loss = 3.227744\n",
      "Epoch 3000: Loss = 3.191901\n",
      "Epoch 3010: Loss = 3.157428\n",
      "Epoch 3020: Loss = 3.123958\n",
      "Epoch 3030: Loss = 3.091225\n",
      "Epoch 3040: Loss = 3.059362\n",
      "Epoch 3050: Loss = 3.028375\n",
      "Epoch 3060: Loss = 3.001462\n",
      "Epoch 3070: Loss = 3.196409\n",
      "Epoch 3080: Loss = 3.064550\n",
      "Epoch 3090: Loss = 2.947344\n",
      "Epoch 3100: Loss = 2.893237\n",
      "Epoch 3110: Loss = 2.867679\n",
      "Epoch 3120: Loss = 2.834704\n",
      "Epoch 3130: Loss = 2.810067\n",
      "Epoch 3140: Loss = 2.785549\n",
      "Epoch 3150: Loss = 2.761360\n",
      "Epoch 3160: Loss = 2.737765\n",
      "Epoch 3170: Loss = 2.714689\n",
      "Epoch 3180: Loss = 2.692091\n",
      "Epoch 3190: Loss = 2.669949\n",
      "Epoch 3200: Loss = 2.648216\n",
      "Epoch 3210: Loss = 2.626894\n",
      "Epoch 3220: Loss = 2.605968\n",
      "Epoch 3230: Loss = 2.585506\n",
      "Epoch 3240: Loss = 2.573622\n",
      "Epoch 3250: Loss = 3.279363\n",
      "Epoch 3260: Loss = 2.600111\n",
      "Epoch 3270: Loss = 2.597305\n",
      "Epoch 3280: Loss = 2.516873\n",
      "Epoch 3290: Loss = 2.474598\n",
      "Epoch 3300: Loss = 2.457729\n",
      "Epoch 3310: Loss = 2.440847\n",
      "Epoch 3320: Loss = 2.422660\n",
      "Epoch 3330: Loss = 2.406434\n",
      "Epoch 3340: Loss = 2.390048\n",
      "Epoch 3350: Loss = 2.374127\n",
      "Epoch 3360: Loss = 2.358411\n",
      "Epoch 3370: Loss = 2.342923\n",
      "Epoch 3380: Loss = 2.327667\n",
      "Epoch 3390: Loss = 2.312628\n",
      "Epoch 3400: Loss = 2.297802\n",
      "Epoch 3410: Loss = 2.283182\n",
      "Epoch 3420: Loss = 2.268764\n",
      "Epoch 3430: Loss = 2.254546\n",
      "Epoch 3440: Loss = 2.240520\n",
      "Epoch 3450: Loss = 2.226682\n",
      "Epoch 3460: Loss = 2.213043\n",
      "Epoch 3470: Loss = 2.200459\n",
      "Epoch 3480: Loss = 2.312032\n",
      "Epoch 3490: Loss = 2.191311\n",
      "Epoch 3500: Loss = 2.237885\n",
      "Epoch 3510: Loss = 2.205300\n",
      "Epoch 3520: Loss = 2.159664\n",
      "Epoch 3530: Loss = 2.129913\n",
      "Epoch 3540: Loss = 2.114089\n",
      "Epoch 3550: Loss = 2.103336\n",
      "Epoch 3560: Loss = 2.091617\n",
      "Epoch 3570: Loss = 2.080149\n",
      "Epoch 3580: Loss = 2.069116\n",
      "Epoch 3590: Loss = 2.058134\n",
      "Epoch 3600: Loss = 2.047308\n",
      "Epoch 3610: Loss = 2.036606\n",
      "Epoch 3620: Loss = 2.026018\n",
      "Epoch 3630: Loss = 2.015545\n",
      "Epoch 3640: Loss = 2.005184\n",
      "Epoch 3650: Loss = 1.994935\n",
      "Epoch 3660: Loss = 1.984795\n",
      "Epoch 3670: Loss = 1.974761\n",
      "Epoch 3680: Loss = 1.964834\n",
      "Epoch 3690: Loss = 1.955009\n",
      "Epoch 3700: Loss = 1.945287\n",
      "Epoch 3710: Loss = 1.935669\n",
      "Epoch 3720: Loss = 1.926335\n",
      "Epoch 3730: Loss = 1.943656\n",
      "Epoch 3740: Loss = 2.723461\n",
      "Epoch 3750: Loss = 2.083195\n",
      "Epoch 3760: Loss = 1.922077\n",
      "Epoch 3770: Loss = 1.886224\n",
      "Epoch 3780: Loss = 1.873428\n",
      "Epoch 3790: Loss = 1.865807\n",
      "Epoch 3800: Loss = 1.858063\n",
      "Epoch 3810: Loss = 1.849267\n",
      "Epoch 3820: Loss = 1.840661\n",
      "Epoch 3830: Loss = 1.832675\n",
      "Epoch 3840: Loss = 1.824711\n",
      "Epoch 3850: Loss = 1.816831\n",
      "Epoch 3860: Loss = 1.809030\n",
      "Epoch 3870: Loss = 1.801301\n",
      "Epoch 3880: Loss = 1.793638\n",
      "Epoch 3890: Loss = 1.786043\n",
      "Epoch 3900: Loss = 1.778512\n",
      "Epoch 3910: Loss = 1.771048\n",
      "Epoch 3920: Loss = 1.763647\n",
      "Epoch 3930: Loss = 1.756310\n",
      "Epoch 3940: Loss = 1.749035\n",
      "Epoch 3950: Loss = 1.741824\n",
      "Epoch 3960: Loss = 1.734672\n",
      "Epoch 3970: Loss = 1.727582\n",
      "Epoch 3980: Loss = 1.720554\n",
      "Epoch 3990: Loss = 1.713689\n",
      "Epoch 4000: Loss = 1.721005\n",
      "Epoch 4010: Loss = 2.806676\n",
      "Epoch 4020: Loss = 1.715810\n",
      "Epoch 4030: Loss = 1.687596\n",
      "Epoch 4040: Loss = 1.683455\n",
      "Epoch 4050: Loss = 1.678094\n",
      "Epoch 4060: Loss = 1.671673\n",
      "Epoch 4070: Loss = 1.664267\n",
      "Epoch 4080: Loss = 1.656833\n",
      "Epoch 4090: Loss = 1.650333\n",
      "Epoch 4100: Loss = 1.644419\n",
      "Epoch 4110: Loss = 1.638449\n",
      "Epoch 4120: Loss = 1.632548\n",
      "Epoch 4130: Loss = 1.626706\n",
      "Epoch 4140: Loss = 1.620906\n",
      "Epoch 4150: Loss = 1.615149\n",
      "Epoch 4160: Loss = 1.609435\n",
      "Epoch 4170: Loss = 1.603762\n",
      "Epoch 4180: Loss = 1.598130\n",
      "Epoch 4190: Loss = 1.592540\n",
      "Epoch 4200: Loss = 1.586990\n",
      "Epoch 4210: Loss = 1.581481\n",
      "Epoch 4220: Loss = 1.576010\n",
      "Epoch 4230: Loss = 1.570579\n",
      "Epoch 4240: Loss = 1.565187\n",
      "Epoch 4250: Loss = 1.559834\n",
      "Epoch 4260: Loss = 1.554526\n",
      "Epoch 4270: Loss = 1.549886\n",
      "Epoch 4280: Loss = 1.648342\n",
      "Epoch 4290: Loss = 1.539141\n",
      "Epoch 4300: Loss = 1.634976\n",
      "Epoch 4310: Loss = 1.584641\n",
      "Epoch 4320: Loss = 1.549209\n",
      "Epoch 4330: Loss = 1.529750\n",
      "Epoch 4340: Loss = 1.517970\n",
      "Epoch 4350: Loss = 1.510641\n",
      "Epoch 4360: Loss = 1.505736\n",
      "Epoch 4370: Loss = 1.501227\n",
      "Epoch 4380: Loss = 1.496500\n",
      "Epoch 4390: Loss = 1.491924\n",
      "Epoch 4400: Loss = 1.487383\n",
      "Epoch 4410: Loss = 1.482874\n",
      "Epoch 4420: Loss = 1.478394\n",
      "Epoch 4430: Loss = 1.473944\n",
      "Epoch 4440: Loss = 1.469521\n",
      "Epoch 4450: Loss = 1.465127\n",
      "Epoch 4460: Loss = 1.460758\n",
      "Epoch 4470: Loss = 1.456418\n",
      "Epoch 4480: Loss = 1.452103\n",
      "Epoch 4490: Loss = 1.447817\n",
      "Epoch 4500: Loss = 1.443556\n",
      "Epoch 4510: Loss = 1.439322\n",
      "Epoch 4520: Loss = 1.435114\n",
      "Epoch 4530: Loss = 1.430934\n",
      "Epoch 4540: Loss = 1.426952\n",
      "Epoch 4550: Loss = 1.442007\n",
      "Epoch 4560: Loss = 2.414223\n",
      "Epoch 4570: Loss = 1.431180\n",
      "Epoch 4580: Loss = 1.421906\n",
      "Epoch 4590: Loss = 1.425693\n",
      "Epoch 4600: Loss = 1.415692\n",
      "Epoch 4610: Loss = 1.403479\n",
      "Epoch 4620: Loss = 1.395948\n",
      "Epoch 4630: Loss = 1.392334\n",
      "Epoch 4640: Loss = 1.388576\n",
      "Epoch 4650: Loss = 1.384705\n",
      "Epoch 4660: Loss = 1.381075\n",
      "Epoch 4670: Loss = 1.377414\n",
      "Epoch 4680: Loss = 1.373797\n",
      "Epoch 4690: Loss = 1.370202\n",
      "Epoch 4700: Loss = 1.366625\n",
      "Epoch 4710: Loss = 1.363067\n",
      "Epoch 4720: Loss = 1.359529\n",
      "Epoch 4730: Loss = 1.356011\n",
      "Epoch 4740: Loss = 1.352512\n",
      "Epoch 4750: Loss = 1.349030\n",
      "Epoch 4760: Loss = 1.345568\n",
      "Epoch 4770: Loss = 1.342125\n",
      "Epoch 4780: Loss = 1.338700\n",
      "Epoch 4790: Loss = 1.335303\n",
      "Epoch 4800: Loss = 1.332430\n",
      "Epoch 4810: Loss = 1.386088\n",
      "Epoch 4820: Loss = 1.642500\n",
      "Epoch 4830: Loss = 1.527552\n",
      "Epoch 4840: Loss = 1.343342\n",
      "Epoch 4850: Loss = 1.315827\n",
      "Epoch 4860: Loss = 1.317442\n",
      "Epoch 4870: Loss = 1.313252\n",
      "Epoch 4880: Loss = 1.306792\n",
      "Epoch 4890: Loss = 1.303688\n",
      "Epoch 4900: Loss = 1.300524\n",
      "Epoch 4910: Loss = 1.297404\n",
      "Epoch 4920: Loss = 1.294365\n",
      "Epoch 4930: Loss = 1.291363\n",
      "Epoch 4940: Loss = 1.288366\n",
      "Epoch 4950: Loss = 1.285389\n",
      "Epoch 4960: Loss = 1.282428\n",
      "Epoch 4970: Loss = 1.279481\n",
      "Epoch 4980: Loss = 1.276547\n",
      "Epoch 4990: Loss = 1.273628\n",
      "Epoch 5000: Loss = 1.270722\n",
      "Training complete.\n",
      "\n",
      "t =  0.0, ŷ(t) = [ 0.  0.  0.  0. -0.]\n",
      "t =  2.5, ŷ(t) = [0.77076226 0.56353146 0.32362628 0.95211756 2.727878  ]\n",
      "t =  5.0, ŷ(t) = [0.80651236 0.51184165 0.36709926 0.878643   2.9930756 ]\n",
      "t =  7.5, ŷ(t) = [0.8319299 0.5367032 0.4049613 0.8191961 3.0014646]\n",
      "t = 10.0, ŷ(t) = [0.8463513  0.5613387  0.43727168 0.7845049  2.9892583 ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Training Loop using Adam (lr = 0.001) on the GPU\n",
    "# ------------------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5000\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    loss_val = compute_loss_vectorized()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs.\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss_val.item():.6f}\")\n",
    "\n",
    "    # Stop if loss is below the threshold.\n",
    "    if loss_val.item() < 1e-6:\n",
    "        print(f\"Stopping training at epoch {epoch} with loss = {loss_val.item():.6f}\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate the trained model at select time points\n",
    "# ------------------------------\n",
    "test_times = [0.0, 2.5, 5.0, 7.5, 10.0]\n",
    "for t in test_times:\n",
    "    t_tensor = torch.tensor(t, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_pred = model(t_tensor)\n",
    "    print(f\"t = {t:4.1f}, ŷ(t) = {y_pred.detach().cpu().numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a seaborn theme\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot each solution component with seaborn's lineplot\n",
    "# for i in range(5):\n",
    "#     sns.lineplot(x=t_plot_np, y=y_plot[:, i], label=f'y[{i}]')\n",
    "\n",
    "# plt.xlabel('t')\n",
    "# plt.ylabel('y')\n",
    "# plt.title('PINN Solution Components over [0,10]')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6711, 0.8113, 0.3215, 0.7965, 2.9876], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ = torch.tensor(10.2, dtype=torch.float32, device=device, requires_grad=True)\n",
    "y_ = model(t_)\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model at select time points and visualize each solution component.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a fine grid over [0,10]\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m t_plot \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m200\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# shape: (200,)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate the model on this grid. (Ensure correct shape by unsqueezing.)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m y_plot \u001b[38;5;241m=\u001b[39m model(t_plot\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# shape: (200, 5)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Evaluate the trained model at select time points and visualize each solution component.\n",
    "# ------------------------------\n",
    "# Create a fine grid over [0,10]\n",
    "t_plot = torch.linspace(0, 10, 200, dtype=torch.float32, device=device)  # shape: (200,)\n",
    "# Evaluate the model on this grid. (Ensure correct shape by unsqueezing.)\n",
    "y_plot = model(t_plot.unsqueeze(1))  # shape: (200, 5)\n",
    "y_plot = y_plot.detach().cpu().numpy()\n",
    "t_plot_np = t_plot.detach().cpu().numpy()\n",
    "\n",
    "# Create a plot for each component y[0]...y[4]\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(5):\n",
    "    plt.plot(t_plot_np, y_plot[:, i], label=f\"y[{i}]\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"PINN Solution Components over [0,10]\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
